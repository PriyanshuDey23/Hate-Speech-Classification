{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Hate-Speech-Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the folder\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Hate-Speech-Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    metric_file_name: Path\n",
    "    tokenizer_path: Path\n",
    "    x_test_data_path: str\n",
    "    y_test_data_path: str\n",
    "    Max_Len: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from Hate_Speech_Classification.constrants import * # Import Everything\n",
    "from Hate_Speech_Classification.utils.common import read_yaml,create_directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params= self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_path = config.model_path,\n",
    "            metric_file_name = config.metric_file_name,\n",
    "            tokenizer_path=config.tokenizer_path,\n",
    "            x_test_data_path=config.x_test_data_path,\n",
    "            y_test_data_path= config.y_test_data_path,\n",
    "            Max_Len=params.Max_Len\n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Hate_Speech_Classification.Logging import logging\n",
    "from Hate_Speech_Classification.constrants import * # Import Everything\n",
    "from Hate_Speech_Classification.Exception import CustomException\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Load the tokenizer from a file (tokenizer.pickle).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open('tokenizer.pickle', 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "            logging.info(\"Tokenizer loaded successfully.\")\n",
    "            return tokenizer\n",
    "        except FileNotFoundError as e:\n",
    "            raise CustomException(f\"Tokenizer file not found: {str(e)}\", sys)\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys) from e\n",
    "        \n",
    "    def save_tokenizer(self, tokenizer):\n",
    "        try:\n",
    "            tokenizer_file_path = os.path.join(self.config.tokenizer_path, 'tokenizer.pickle')\n",
    "            with open(tokenizer_file_path, 'wb') as handle:\n",
    "                pickle.dump(tokenizer, handle)\n",
    "            logging.info(f\"Tokenizer saved successfully at {tokenizer_file_path}.\")\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys) from e\n",
    "\n",
    "    def save_accuracy(self, accuracy):\n",
    "        try:\n",
    "            accuracy_file_path = os.path.join(self.config.metric_file_name, 'accuracy.txt')\n",
    "            with open(accuracy_file_path, 'w') as file:\n",
    "                file.write(f\"Test Accuracy: {accuracy}\\n\")\n",
    "            logging.info(f\"Accuracy saved successfully at {accuracy_file_path}.\")\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys) from e\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        \n",
    "        try:\n",
    "            logging.info(\"Entering into to the evaluate function of Model Evaluation class\")\n",
    "            print(self.config.x_test_data_path)\n",
    "\n",
    "            # Loading test data\n",
    "\n",
    "            x_test = pd.read_csv(self.config.x_test_data_path,index_col=0).squeeze()\n",
    "            print(x_test)\n",
    "            y_test = pd.read_csv(self.config.x_test_data_path,index_col=0).squeeze()\n",
    "            print(y_test)\n",
    "\n",
    "\n",
    "            # Check for NaN values\n",
    "            if x_test.isnull().any():\n",
    "                logging.warning(\"x_test contains NaN values, filling with empty string.\")\n",
    "                x_test = x_test.fillna('')\n",
    "            if y_test.isnull().any():\n",
    "                logging.warning(\"y_test contains NaN values, filling with 0.\")\n",
    "                y_test = pd.to_numeric(y_test, errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "                y_test = y_test.fillna(0)  # Or handle NaNs as needed\n",
    "                logging.info(f\"Unique values in y_test after conversion: {y_test.unique()}\")\n",
    "\n",
    "            # Preprocess test data\n",
    "            x_test = x_test.astype(str)\n",
    "            y_test = y_test.astype(float)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Load the tokenizer\n",
    "            tokenizer = self.load_tokenizer()\n",
    "\n",
    "            # Load trained model\n",
    "            load_model=keras.models.load_model(self.config.model_path)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            # Tokenizing the test data\n",
    "            test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "            test_sequences_matrix = pad_sequences(test_sequences,maxlen=self.config.Max_Len)\n",
    "            \n",
    "\n",
    "\n",
    "            # Evaluate model\n",
    "\n",
    "            accuracy = load_model.evaluate(test_sequences_matrix,y_test)\n",
    "            logging.info(f\"the test accuracy is {accuracy}\")\n",
    "\n",
    "\n",
    "            # Save tokenizer and accuracy\n",
    "            self.save_tokenizer(tokenizer)\n",
    "            self.save_accuracy(accuracy)\n",
    "\n",
    "             # Making predictions\n",
    "\n",
    "            lstm_prediction = load_model.predict(test_sequences_matrix)\n",
    "            res = []\n",
    "            for prediction in lstm_prediction:\n",
    "                if prediction[0] >= 0.5:\n",
    "                    res.append(1)  # Positive class\n",
    "                else:\n",
    "                    res.append(0)  # Negative class\n",
    "\n",
    "            \n",
    "            # Confusion matrix\n",
    "            print(confusion_matrix(y_test,res))\n",
    "            logging.info(f\"the confusion_matrix is {confusion_matrix(y_test,res)} \")\n",
    "            return accuracy\n",
    "        \n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys) from e\n",
    "\n",
    "\n",
    "\n",
    "    def initiate_model_evaluation(self) -> dict:\n",
    "        try:\n",
    "            logging.info(\"Initiating Model Evaluation\")\n",
    "\n",
    "            # Evaluate the current model\n",
    "            trained_model_accuracy = self.evaluate()\n",
    "            \n",
    "            # Log the accuracy for debugging\n",
    "            logging.info(f\"Trained Model Accuracy: {trained_model_accuracy}\")\n",
    "\n",
    "            # If trained_model_accuracy is a tuple or list, assume it returns (loss, accuracy)\n",
    "            if isinstance(trained_model_accuracy, (list, tuple)):\n",
    "                accuracy = trained_model_accuracy[1]  # Assuming accuracy is at index 1\n",
    "            else:\n",
    "                accuracy = trained_model_accuracy  # If it's just a single scalar value\n",
    "            \n",
    "            # Ensure accuracy is a float before comparison\n",
    "            is_model_accepted = float(accuracy) >= 0.5  \n",
    "            logging.info(\"Trained model accepted.\" if is_model_accepted else \"Trained model not accepted.\")\n",
    "\n",
    "            return {\n",
    "                'is_model_accepted': is_model_accepted,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys) from e\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/model_trainer/x_test.csv\n",
      "44200    say would let guy fall ground get wail second ...\n",
      "46339    user funni shoot offic today stay tune pic com...\n",
      "49169    user waooooow hea hu right mean knew jame pers...\n",
      "54130    user user yes except happen side everyon lost ...\n",
      "18801    rt cpabri curious georg curious littl monkey d...\n",
      "                               ...                        \n",
      "19253    rt indykaila exclus wenger gave two yellow car...\n",
      "38605    ð toptag toptag nothappi cri cri tear insta...\n",
      "25895    someon stimul internet theyr alway second gues...\n",
      "56285    day first harvest harummani mango tanamsendiriâ¦\n",
      "20496    realli barnicl oj anthoni weiner mj clown show...\n",
      "Name: tweet, Length: 17024, dtype: object\n",
      "44200    say would let guy fall ground get wail second ...\n",
      "46339    user funni shoot offic today stay tune pic com...\n",
      "49169    user waooooow hea hu right mean knew jame pers...\n",
      "54130    user user yes except happen side everyon lost ...\n",
      "18801    rt cpabri curious georg curious littl monkey d...\n",
      "                               ...                        \n",
      "19253    rt indykaila exclus wenger gave two yellow car...\n",
      "38605    ð toptag toptag nothappi cri cri tear insta...\n",
      "25895    someon stimul internet theyr alway second gues...\n",
      "56285    day first harvest harummani mango tanamsendiriâ¦\n",
      "20496    realli barnicl oj anthoni weiner mj clown show...\n",
      "Name: tweet, Length: 17024, dtype: object\n",
      "532/532 [==============================] - 33s 61ms/step - loss: 1.6741 - accuracy: 0.6133\n",
      "532/532 [==============================] - 33s 61ms/step\n",
      "[[10441  6583]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    model_eval_config = config_manager.get_model_evaluation_config()\n",
    "    evaluation = ModelEvaluation(model_eval_config)\n",
    "    evaluation.initiate_model_evaluation()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
